
defaults:
  - model@model: ???
  - data@_global_: features
  - _self_

# mlflow
path_to_mlflow: mlruns
experiment_name: ???

# datasets
dataset_name: SM_v4 # to log to mlflow for bookkeeping
datasets: 
  train:
    SM_v4: # will take all files from "train" subfolder of this dataset
      path_to_dataset: ./datasets
      tau_types: ["tau", "e", "mu", "jet"]
  val:
    SM_v4: # will take all files from "val" subfolder of this dataset
      path_to_dataset: ./datasets
      tau_types: ["tau", "e", "mu", "jet"]  

# TF dataset formation
tf_dataset_cfg:
  n_threads: 5 # without controlling it may throw thread limit exception
  smart_batching_step: 10
  sequence_length_dist_start: 0
  sequence_length_dist_end: 300 
  shuffle_buffer_size: 40000 # null to not shuffle
  shuffle_smart_buffer_size: 1000
  cache: null # null to not cache the training dataset
  train_batch_size: 128
  val_batch_size: 128
  classes: ["tau", "e", "mu", "jet"] # will pick only those labels (in this order)

# training
schedule: null # custom // null 
warmup_steps: 4000 # only for schedule=custom, max LR = 1/sqrt(dim_model) * 1/sqrt(warmup_steps)
learning_rate: 0.0001 # only for schedule=null
optimiser: sgd
momentum: 0.9 # only for sgd
nesterov: True # only for sgd
n_epochs: 1
min_delta: 0.0001
patience: 3

# gpu
gpu_id: 0
memory_limit: 8 # in Gb
